# Vercel Chat SDK Guide: Building Advanced AI Chat Applications

## Introduction

The Vercel AI SDK is a powerful toolkit that allows you to build AI-powered chat applications with ease. This guide will walk you through the fundamentals of the SDK, how to integrate various LLMs, and how to create custom artifacts for specialized content.

## Table of Contents

1. [Getting Started with Vercel AI SDK](#getting-started-with-vercel-ai-sdk)
2. [Core Concepts](#core-concepts)
3. [Integrating Different LLMs](#integrating-different-llms)
4. [Creating Custom Artifacts](#creating-custom-artifacts)
5. [Advanced Features](#advanced-features)
6. [Best Practices](#best-practices)
7. [Troubleshooting](#troubleshooting)

## Getting Started with Vercel AI SDK

### Installation

To start using the Vercel AI SDK, create a new Next.js project or use an existing one and install the required packages:

```bash
npm install ai @vercel/ai
# or
pnpm add ai @vercel/ai
# or
yarn add ai @vercel/ai
```

### Basic Chat Implementation

The simplest implementation involves creating an API route that uses the AI SDK to handle chat requests:

```typescript
// app/api/chat/route.ts
import { StreamingTextResponse, Message } from 'ai';
import { OpenAIStream } from 'ai/streams';

export async function POST(req: Request) {
  const { messages } = await req.json();

  // Create a stream from OpenAI
  const stream = OpenAIStream({
    model: 'gpt-3.5-turbo',
    messages,
    temperature: 0.7,
  });

  // Return a streaming response
  return new StreamingTextResponse(stream);
}
```

Then on the client side:

```typescript
// app/page.tsx
'use client';

import { useChat } from 'ai/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  return (
    <div>
      <div>
        {messages.map(message => (
          <div key={message.id}>
            <strong>{message.role === 'user' ? 'You: ' : 'AI: '}</strong>
            {message.content}
          </div>
        ))}
      </div>

      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Say something..."
        />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}
```

## Core Concepts

### Messages

Messages are the fundamental building blocks of a conversation. Each message has:
- A `role` (user, assistant, or system)
- `content` (the text of the message)
- Optional `id` and other metadata

### Streaming

The AI SDK is built around streaming responses, which provide a more responsive user experience by showing responses as they're generated rather than waiting for the entire response to complete.

### Artifacts

Artifacts are specialized content types that can be generated by AI. The Vercel AI SDK supports various types of artifacts:
- Text
- Code
- Image
- Sheet
- Custom artifacts (like our LaTeX implementation)

## Integrating Different LLMs

The Vercel AI SDK supports multiple language models from different providers.

### Using OpenAI

```typescript
import { openai } from '@ai-sdk/openai';

// Initialize with your API key
const completion = await openai({
  apiKey: process.env.OPENAI_API_KEY,
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello, how are you?' }]
});
```

### Using Google's Gemini (formerly known as xAI)

```typescript
import { google } from '@ai-sdk/google';

export const myProvider = customProvider({
  languageModels: {
    'chat-model': google('gemini-2.0-flash'),
    'title-model': google('gemini-2.0-flash'),
    'artifact-model': google('gemini-2.0-flash'),
    'latex-model': google('gemini-2.0-flash'),
  }
});
```

To set this up, you need to install the Google provider:

```bash
npm install @ai-sdk/google
```

And configure it with your API key in an environment variable:

```
GOOGLE_API_KEY=your_api_key_here
```

### Using Anthropic Claude

```typescript
import { anthropic } from '@ai-sdk/anthropic';

const completion = await anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
  model: 'claude-3-haiku',
  messages: [{ role: 'user', content: 'Hello, Claude!' }]
});
```

### Creating a Custom Provider

You can create a custom provider that combines different models for different purposes:

```typescript
import { customProvider } from 'ai';
import { google } from '@ai-sdk/google';
import { openai } from '@ai-sdk/openai';

export const myProvider = customProvider({
  languageModels: {
    'chat-model': google('gemini-2.0-flash'),
    'image-model': openai('dall-e-3'),
    'code-model': anthropic('claude-3-opus'),
  }
});
```

## Creating Custom Artifacts

One of the most powerful features of the Vercel AI SDK is the ability to create custom artifacts. Here we'll walk through how we implemented the LaTeX artifact.

### Step 1: Define the Client-Side Artifact

First, create a client-side artifact component:

```typescript
// artifacts/latex/client.tsx
import { Artifact } from '@/components/create-artifact';
import { EditorView } from '@codemirror/view';
import { EditorState, Transaction } from '@codemirror/state';
import { oneDark } from '@codemirror/theme-one-dark';
import { basicSetup } from 'codemirror';
import { markdown } from '@codemirror/lang-markdown';
import { syntaxHighlighting, HighlightStyle } from '@codemirror/language';
import { tags } from '@lezer/highlight';

// Custom highlighting for LaTeX syntax
const latexHighlightStyle = HighlightStyle.define([
  { tag: tags.keyword, color: '#f92672' },
  { tag: tags.string, color: '#e6db74' },
  // More styling rules...
]);

// Preview component for small window display
export const LatexPreview = ({ content }: { content: string }) => {
  return (
    <div className="p-4 font-mono text-sm overflow-auto max-h-[300px] bg-zinc-800 rounded">
      <pre>{content}</pre>
    </div>
  );
};

export const latexArtifact = new Artifact<'latex', LatexArtifactMetadata>({
  kind: 'latex',
  description: 'Useful for LaTeX documents with mathematical equations and formatting.',
  initialize: async ({ setMetadata }) => {
    setMetadata({
      suggestions: [],
    });
  },
  onStreamPart: ({ streamPart, setArtifact }) => {
    if (streamPart.type === 'latex-delta') {
      const newContent = streamPart.content as string;

      setArtifact((draftArtifact) => {
        return {
          ...draftArtifact,
          content: newContent,
          isVisible: true,
          status: 'streaming' as const,
        };
      });
    }
  },
  content: function Content({
    mode,
    status,
    content,
    isCurrentVersion,
    currentVersionIndex,
    onSaveContent,
    getDocumentContentById,
    isLoading,
    metadata,
  }) {
    // Component rendering logic
    return (
      <div className="px-1">
        <div className="w-full">
          <LatexEditor
            content={content}
            suggestions={metadata ? metadata.suggestions : []}
            isCurrentVersion={isCurrentVersion}
            currentVersionIndex={currentVersionIndex}
            status={status}
            onSaveContent={onSaveContent}
          />
        </div>
      </div>
    );
  },
  actions: [
    // Action buttons for the artifact
    {
      icon: <CopyIcon size={18} />,
      description: 'Copy to clipboard',
      onClick: ({ content }) => {
        navigator.clipboard.writeText(content);
        toast.success('Copied to clipboard!');
      },
    },
    // More actions...
  ],
  // Toolbar options
});
```

### Step 2: Create the Server-Side Handler

Next, create a server-side handler to generate LaTeX content:

```typescript
// artifacts/latex/server.ts
import { z } from 'zod';
import { streamObject } from 'ai';
import { myProvider } from '@/lib/ai/providers';
import { createDocumentHandler } from '@/lib/artifacts/server';

export const latexDocumentHandler = createDocumentHandler<'latex'>({
  kind: 'latex',
  onCreateDocument: async ({ title, dataStream }) => {
    let draftContent = '';

    const { fullStream } = streamObject({
      model: myProvider.languageModel('latex-model'),
      system: `Create a complete and well-structured LaTeX document...`,
      prompt: title,
      schema: z.object({
        latex: z.string(),
      }),
    });

    for await (const delta of fullStream) {
      if (delta.type === 'object') {
        const { object } = delta;
        const { latex } = object;

        if (latex) {
          dataStream.writeData({
            type: 'latex-delta',
            content: latex ?? '',
          });

          draftContent = latex;
        }
      }
    }

    return draftContent;
  },
  onUpdateDocument: async ({ document, description, dataStream }) => {
    // Similar logic for updating documents
  },
});
```

### Step 3: Register Your Artifact

You need to register your artifact with the SDK:

```typescript
// components/artifact.tsx
import { latexArtifact } from '@/artifacts/latex/client';

export const artifactDefinitions = [
  textArtifact,
  codeArtifact,
  imageArtifact,
  sheetArtifact,
  latexArtifact, // Add your new artifact here
];
```

### Step 4: Update the Document Preview Component

Make sure your artifact can be properly displayed in the preview:

```typescript
// components/document-preview.tsx
import { LatexPreview } from '@/artifacts/latex/client';

const DocumentContent = ({ document }: { document: Document }) => {
  // ...
  return (
    <div className={containerClassName}>
      {document.kind === 'text' ? (
        <Editor {...commonProps} onSaveContent={() => {}} />
      ) : document.kind === 'code' ? (
        // Code editor
      ) : document.kind === 'latex' ? (
        <LatexPreview content={document.content ?? ''} />
      ) : null}
    </div>
  );
};
```

## Advanced Features

### System Prompts

System prompts help guide the behavior of the AI. For example, with our LaTeX artifact:

```typescript
const { fullStream } = streamObject({
  model: myProvider.languageModel('latex-model'),
  system: `Create a complete and well-structured LaTeX document based on the given title or description.
Include proper LaTeX syntax with all necessary preamble commands.
Your document should:
1. Start with a proper document class and preamble
2. Include appropriate packages for mathematical notation
3. Use proper LaTeX environments for theorems, proofs, equations, etc.
4. Include sample content that demonstrates the capabilities of LaTeX
5. Format mathematical equations correctly using $ and $$ notation for inline and display math
6. Be well-commented to explain the purpose of different LaTeX commands`,
  prompt: title,
  schema: z.object({
    latex: z.string(),
  }),
});
```

### Stream Handling

The AI SDK provides robust handling of streaming data. Here's how we handle streaming for artifacts:

```typescript
export function DataStreamHandler({ id }: { id: string }) {
  const { data: dataStream } = useChat({ id });
  const { artifact, setArtifact, setMetadata } = useArtifact();
  const lastProcessedIndex = useRef(-1);

  useEffect(() => {
    if (!dataStream?.length) return;

    const newDeltas = dataStream.slice(lastProcessedIndex.current + 1);
    lastProcessedIndex.current = dataStream.length - 1;

    (newDeltas as DataStreamDelta[]).forEach((delta: DataStreamDelta) => {
      const artifactDefinition = artifactDefinitions.find(
        (artifactDefinition) => artifactDefinition.kind === artifact.kind,
      );

      if (artifactDefinition?.onStreamPart) {
        artifactDefinition.onStreamPart({
          streamPart: delta,
          setArtifact,
          setMetadata,
        });
      }

      // Handle other delta types...
    });
  }, [dataStream, setArtifact, setMetadata, artifact]);

  return null;
}
```

## Best Practices

### Handling Errors

Always implement robust error handling in your AI applications:

```typescript
try {
  const result = await model.generate(...);
  // Process result
} catch (error) {
  console.error('AI generation failed:', error);
  // Provide fallback or error message
}
```

### Rate Limiting

Be mindful of rate limits from AI providers. Implement appropriate caching and throttling:

```typescript
import { rateLimit } from 'express-rate-limit';

const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // limit each IP to 100 requests per windowMs
});

app.use('/api/chat', limiter);
```

### Prompt Engineering

Write clear, specific prompts for better results:

- Be specific about the format you want
- Provide examples of desired output
- Break down complex tasks into steps
- Use system messages to set context

## Troubleshooting

### Common Issues

1. **Stream not showing**: Ensure you're using StreamingTextResponse and properly handling stream data on the client
2. **Artifacts not displaying**: Check registration of artifacts and correct implementation of onStreamPart
3. **Type errors**: Ensure your type definitions match the SDK's expectations
4. **API Key errors**: Verify your environment variables are correctly set

### Debugging

Use the browser console and server logs to debug streaming issues:

```typescript
onStreamPart: ({ streamPart, setArtifact }) => {
  console.log('Stream part:', streamPart);
  // Handler logic
}
```

## Conclusion

The Vercel AI SDK provides a powerful framework for building AI-powered chat applications. By understanding the core concepts and following the patterns outlined in this guide, you can create sophisticated applications with custom artifacts tailored to your specific needs.

## Additional Resources

- [Vercel AI SDK Documentation](https://sdk.vercel.ai/docs)
- [OpenAI Documentation](https://platform.openai.com/docs/api-reference)
- [Google AI Documentation](https://ai.google.dev/docs)
- [LaTeX Documentation](https://www.latex-project.org/help/documentation/)